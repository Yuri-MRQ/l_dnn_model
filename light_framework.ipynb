{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526f17f1",
   "metadata": {},
   "source": [
    "## Many thanks to Andrew W. Task and his Great book Grokking Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48c5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b85c1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if id is None:\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for father in creators:\n",
    "                if self.id not in father.children:\n",
    "                    father.children[self.id] = 1\n",
    "                else:\n",
    "                    father.children[self.id] += 1\n",
    "                    \n",
    "    def all_children_grads_accounter_for(self):\n",
    "        #in the backward function every time\n",
    "        #we backprop through a children we decrease\n",
    "        #the count, so we can confirme if the Tensor\n",
    "        #has recived the correct number of grad\n",
    "        \n",
    "        for children_id, count in self.children.items():\n",
    "            if count != 0:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if self.autograd:\n",
    "            \n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "                \n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                    \n",
    "            if self.grad is None:\n",
    "                if type(grad) == np.ndarray:\n",
    "                    self.grad = grad.copy()\n",
    "                else:\n",
    "                    self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "\n",
    "            if self.creators is not None and (self.all_children_grads_accounter_for() or grad_origin is None):\n",
    "                \n",
    "                if self.creation_op == \"add\":\n",
    "                    self.creators[0].backward(grad, self)\n",
    "                    self.creators[1].backward(grad, self)\n",
    "                    \n",
    "                if self.creation_op == \"neg\":\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                \n",
    "                if self.creation_op == \"sub\":\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if self.creation_op == \"mul\":\n",
    "                    new = self.grad.data * self.creators[1].data\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad.data * self.creators[0].data\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if self.creation_op == \"mm\":\n",
    "                    activation = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    activation.backward(new)\n",
    "                    new = self.grad.transpose().mm(activation).transpose()\n",
    "                    weights.backward(new)\n",
    "                    \n",
    "                if self.creation_op == \"transpose\":\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                    \n",
    "                if \"sum\" in self.creation_op:\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand_dimension(dim,ds))\n",
    "                    \n",
    "                if \"expand\" in self.creation_op:\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "    def sum(self, dimension):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dimension),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"sum_\" +str(dimension))\n",
    "        return Tensor(self.data.sum(dimension))\n",
    "    \n",
    "    def expand_dimension(self, dimension, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dimension, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        \n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"expand_\"+str(dimension))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                         autograd=True,\n",
    "                         creators=[self, x],\n",
    "                         creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "358f3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.grad.data *= 0\n",
    "    \n",
    "    def step(self, zero=True):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.data -= parameter.grad.data*self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                parameter.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4ce1559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameter(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_output):\n",
    "        super().__init__()\n",
    "        w = np.random.randn(n_inputs, n_output)*np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(w, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_output), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand_dimension(0, len(input.data))\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers = list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "    def add(self, layer):\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameter(self):\n",
    "        parameters = list()\n",
    "        for layer in self.layers:\n",
    "            parameters +=layer.get_parameter()\n",
    "        return parameters\n",
    "            \n",
    "class MSELoss(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, predictions, target):\n",
    "        return ((predictions - target)*(predictions - target)).sum(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7a7d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c551821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.75285243]\n",
      "[3.62819194]\n",
      "[1.83000021]\n",
      "[0.96674948]\n",
      "[0.55857984]\n",
      "[0.34433998]\n",
      "[0.22350151]\n",
      "[0.15043032]\n",
      "[0.10386531]\n",
      "[0.07297629]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "\n",
    "optmizer = SGD(parameters=model.get_parameter(), alpha=0.05)\n",
    "criterion = MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optmizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d8f7fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1763"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5 + 2*6 +3*6 + 4*9 + 5*18 + 6*25 + 7*31 + 8*120 + 9*25 + 10*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf20f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
