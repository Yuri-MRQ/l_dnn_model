{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = (1/(1+np.exp(-z)))\n",
    "    cache = z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Compute the relu of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,z)\n",
    "    assert(A.shape == z.shape)\n",
    "    cache = z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Compute the tanh of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    A = (2/(1+ np.exp(-2*z)))-1\n",
    "    cache = z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \n",
    "    \"\"\"Compute the softmax\n",
    "    \n",
    "    Arguments:\n",
    "    z- A scalar or numpy arry of any size.\n",
    "    \n",
    "    Return:\n",
    "    sm -- softmax(z)\n",
    "    \"\"\"\n",
    "    z = z - np.max(z)\n",
    "    A = np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True))\n",
    "    cache = z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_sigmoid(dA, Z):\n",
    "    \"\"\"\n",
    "    Compute the derivative of sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    dA --\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_relu(dA, Z):\n",
    "    \"\"\"\n",
    "    Compute the derivative of relu of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    dA --\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    \n",
    "    dZ = np.array(dA, copy=True)  \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_tanh(dA, Z):\n",
    "    \"\"\"\n",
    "    Compute the derivative of tanh of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    dA -- \n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    dZ = dAL*(1 - np.power(np.tanh(Z), 2))\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_softmax(A, Y, Z):\n",
    "    \n",
    "    \"\"\"Compute the derivative of softmax\n",
    "    \n",
    "    Arguments:\n",
    "    z- A scalar or numpy arry of any size.\n",
    "    A -- \n",
    "    Y -- \n",
    "    Z -- \n",
    "    \n",
    "    Return:\n",
    "    sm -- softmax(z)\n",
    "    \"\"\"\n",
    "    dZ = A - Y\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(model_shape, activation):\n",
    "    \"\"\"\n",
    "    Inizialize the model paramaters w, b\n",
    "    \n",
    "    Arguments:\n",
    "    model_shape -- A list with all the layers shapes. Ex.: (10, 5, 4, 1), in this example we have a 3-layer model with \n",
    "    2 hidden layer (5 units and 4 units) and 1 output layer (binarie)\n",
    "    \n",
    "    Return:\n",
    "    parameters -- return a dictionare of paramaters, the key is the paramater + l (layer number)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(model_shape)):\n",
    "        \n",
    "        if activation[-1] == 'sigmoid':\n",
    "            parameters['W'+str(l)] = np.random.randn(model_shape[l], model_shape[l-1])/ np.sqrt(model_shape[l-1])\n",
    "        else:\n",
    "            parameters['W'+str(l)] = np.random.randn(model_shape[l], model_shape[l-1]) * 0.01\n",
    "        \n",
    "        parameters['b'+str(l)] = np.zeros((model_shape[l], 1))\n",
    "        \n",
    "        assert(parameters['W'+str(l)].shape == (model_shape[l], model_shape[l-1]))\n",
    "        assert(parameters['b'+str(l)].shape == (model_shape[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_foward(A, W, b):\n",
    "    Z = np.dot(W, A)+b\n",
    "    cache = (A, W, b)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_foward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, linear_cache = linear_foward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_foward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'tanh':\n",
    "        Z, linear_cache = linear_foward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "    else:\n",
    "        Z, linear_cache = linear_foward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_prop(X, parameters, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    X -- array of inputs\n",
    "    parameters -- dict of parameters (W1, W2 ... WL), (b1, b2 ... bL)\n",
    "    activation -- list of layers activations\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    AL -- last A from the output\n",
    "    cache -- list with all caches\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    A = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        \n",
    "        #linear foward propagation\n",
    "        A, cache = activation_foward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation[l-1])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    #last layer\n",
    "    AL, cache = activation_foward(A,  parameters['W'+str(L)], parameters['b'+str(L)], activation[L-1])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (parameters['W'+str(L)].shape[0],X.shape[1]))\n",
    "    \n",
    "    return AL, caches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_comput(AL, Y, activation):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \n",
    "    \"\"\"\n",
    "    #if last layer == softmax\n",
    "    L = len(activation)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    if activation[L-1] == 'softmax':\n",
    "        cost = -1/m * np.sum(np.multiply(Y, np.log(AL)))    \n",
    "        \n",
    "    else:\n",
    "\n",
    "        cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "        \n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == (dZ.shape[0], 1))\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(dA, Y, A, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    Z = activation_cache\n",
    "    if activation == 'relu':\n",
    "        dZ = back_relu(dA, Z)\n",
    "        dW, db, dA_prev = linear_backward(dZ,linear_cache)\n",
    "\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = back_sigmoid(dA, Z)\n",
    "        dW, db, dA_prev = linear_backward(dZ,linear_cache)\n",
    "\n",
    "    elif activation == 'tanh':\n",
    "        dZ = back_tanh(dA, Z)\n",
    "        dW, db, dA_prev = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    else:\n",
    "        dZ = back_softmax(A, Y, Z)\n",
    "        dW, db, dA_prev = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    assert(dZ.shape == Z.shape)\n",
    "        \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Y, AL, caches, activation):\n",
    "    \"\"\"\n",
    "    Calculate backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    Y -- True labels\n",
    "    AL -- Model lavel predictions\n",
    "    caches -- cache contains z, w, b, 0 index\n",
    "    activation -- list of layers activations\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    if activation[L-1] == 'sigmoid':\n",
    "        #derivative of cost with respect to AL\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    else:\n",
    "        dAL = 0\n",
    "    #We have to start from the last layer, L in this case, going down to the first layer 0. \n",
    "    #For this we have to initiate with the last layer cache\n",
    "    current_cache = caches[L-1]\n",
    "    #unpacking cache\n",
    "\n",
    "    grads['dW'+str(L)], grads['db'+str(L)], grads['dA'+str(L-1)] =  activation_backward(dAL, Y, AL, current_cache, activation[L-1])\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        #unpacking cache\n",
    "        \n",
    "        linear_cache, activation_cache = current_cache\n",
    "        A_prev, W, b= linear_cache\n",
    "        Z = activation_cache\n",
    "        \n",
    "        grads['dW'+str(l+1)], grads['db'+str(l+1)], grads['dA'+str(l)]  = activation_backward(grads['dA'+str(l+1)], Y, AL, current_cache, activation[l])    \n",
    "        \n",
    "        \n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate*grads['db'+str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, Aurélien Géron\n",
    "def print_status_bar(iteration, print_time, total, loss, acc, metrics=None):\n",
    "    metrics_loss = \" - \".join([\"{}: {:.4f}\".format(m, np.squeeze(loss[m][int(iteration/print_time)])) for m in loss ])\n",
    "    metric_acc = \" - \".join([\"{}: {:.4f}\".format(m, np.squeeze(acc[m][int(iteration/print_time)])) for m in acc ])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{}  - \".format(iteration, total) + metrics_loss + ' / ' + metric_acc, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_dnn(X_train, Y_train, X_dev, Y_dev, learning_rate, num_iterations, model_shape, activation, print_cost=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Model training\n",
    "    \n",
    "    Arguments:\n",
    "    X -- np.array with the inputs\n",
    "    Y -- true labels\n",
    "    learning_rate -- for update the parameters - float\n",
    "    num_iterations -- number of times the paramater going to be update - int\n",
    "    model_shape -- python list, the len(model_shape) is the number of layer and the model_shape[i] is\n",
    "    the number of units\n",
    "    activation -- python list with the activation function for the layers\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    parameters -- model parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = {\"train_cost\" : [], \"dev_cost\" : []}  \n",
    "    acc = {\"train_acc\" : [], \"dev_acc\" : []}\n",
    "    #inizialize paramater\n",
    "    parameters = init_parameters(model_shape, activation)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        #foward prop\n",
    "        AL, caches = foward_prop(X_train, parameters, activation)\n",
    "        AL_dev, caches_dev = foward_prop(X_dev, parameters, activation)\n",
    "        #compute cost\n",
    "        cost_train = cost_comput(AL, Y_train, activation)\n",
    "        cost_dev = cost_comput(AL_dev, Y_dev, activation)\n",
    "        #back prop\n",
    "        grads = back_prop(Y_train, AL, caches, activation)\n",
    "        #parameters update\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        #predict onto train and dev dataset\n",
    "        pred_train, probs_train = predict(X_train, parameters, activation)\n",
    "        pred_dev, probs_dev = predict(X_dev, parameters, activation)\n",
    "        train_acc = evaluate_prediction(pred_train, Y_train)\n",
    "        dev_acc = evaluate_prediction(pred_dev, Y_dev)\n",
    "        # Print the cost every 100 training example\n",
    "        if verbose:\n",
    "            if print_cost and i % 10 == 0:\n",
    "                costs['train_cost'].append(cost_train)\n",
    "                costs['dev_cost'].append(cost_dev)\n",
    "                acc[\"train_acc\"].append(train_acc)\n",
    "                acc[\"dev_acc\"].append(dev_acc)\n",
    "                print_status_bar(i, 10, num_iterations, loss=costs, acc=acc, metrics=None)\n",
    "                \n",
    "    # plot the cost\n",
    "    fig,ax = plt.subplots(1)\n",
    "    l1, = ax.plot(np.squeeze(costs['train_cost']), color='blue')\n",
    "    l2, = ax.plot(np.squeeze(costs['dev_cost']), color='red')\n",
    "    l1.set_label('train_cost')\n",
    "    l2.set_label('dev_cost')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #plot the acc\n",
    "    fig,ax = plt.subplots(1)\n",
    "    l1, = ax.plot(np.squeeze(acc['train_acc']), color='blue')\n",
    "    l2, = ax.plot(np.squeeze(acc['dev_acc']), color='red')\n",
    "    l1.set_label('train_acc')\n",
    "    l2.set_label('dev_acc')\n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters, activation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpd by this great git hub https://charleslow.github.io/softmax_from_scratch/\n",
    "def predict(X, parameters, activation):\n",
    "    # Forward propagation\n",
    "    probabilities, caches = foward_prop(X, parameters, activation)\n",
    "    \n",
    "    # Calculate Predictions (the highest probability for a given example is coded as 1, otherwise 0)\n",
    "    predictions = (probabilities == np.amax(probabilities, axis=0, keepdims=True))\n",
    "    predictions = predictions.astype(float)\n",
    "\n",
    "    return predictions, probabilities\n",
    "\n",
    "def evaluate_prediction(predictions, Y):\n",
    "    m = Y.shape[1]\n",
    "    predictions_class = predictions.argmax(axis=0).reshape(1, m)\n",
    "    Y_class = Y.argmax(axis=0).reshape(1, m)\n",
    "    \n",
    "    return np.sum((predictions_class == Y_class) / (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
